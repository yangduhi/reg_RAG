import asyncio
import hashlib
import shutil
from pathlib import Path
from typing import List, Tuple, Optional, Dict

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from rich.traceback import install

from src.core.config import settings
from src.core.logging import logger
from src.ingestion.db_state import DatabaseStateManager, StatusEnum
from src.ingestion.loaders import LoaderFactory
from src.rag.vectorstore import VectorStoreManager

# Rich traceback í™œì„±í™”ë¡œ ë””ë²„ê¹… ê°€ì‹œì„± í–¥ìƒ
install(show_locals=True, width=120)


class IngestionPipeline:
    """
    RAG ì‹œìŠ¤í…œì˜ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ì„ ì¡°ì •í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

    ì´ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì—”ë“œ íˆ¬ ì—”ë“œ í”„ë¡œì„¸ìŠ¤ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:
    1. êµ¬ì„±ëœ ë””ë ‰í† ë¦¬ì—ì„œ ëŒ€ìƒ ê·œì • íŒŒì¼(XML, PDF) ì‹ë³„.
    2. ì¦ë¶„ ì—…ë°ì´íŠ¸(Incremental Update)ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ íŒŒì¼ ë³€ê²½ ì‚¬í•­(SHA256 í•´ì‹œ) ì¶”ì .
    3. ì „ìš© ë¡œë”(Loader)ë¥¼ ì‚¬ìš©í•œ íŒŒì¼ ë¡œë”© ë° íŒŒì‹±.
    4. í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°ì  ì²­í¬(Recursive Character Chunk)ë¡œ ë¶„í• .
    5. ì²­í¬ì— ë¬¸ë§¥ ë©”íƒ€ë°ì´í„°(ê·œì • ID, ì œëª© ë“±) ì£¼ì….
    6. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì²­í¬ ì¸ë±ì‹±.
    7. ìˆ˜ì§‘ ìƒíƒœ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸.

    Attributes:
        vstore (VectorStoreManager): ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤.
        db_state_manager (DatabaseStateManager): íŒŒì¼ ì²˜ë¦¬ ìƒíƒœ ê´€ë¦¬ì.
        splitter (RecursiveCharacterTextSplitter): í…ìŠ¤íŠ¸ êµ¬ì¡° ê¸°ë°˜ì˜ ë¶„í• ê¸°.
    """

    def __init__(self) -> None:
        """
        IngestionPipelineì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ê´€ë¦¬ìë¥¼ ì„¤ì •í•˜ê³ , í…ìŠ¤íŠ¸ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ëŠ”
        RecursiveCharacterTextSplitterë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        self.vstore: Optional[VectorStoreManager] = None
        self.db_state_manager = DatabaseStateManager()

        logger.info(f"ğŸ› ï¸ RecursiveCharacterTextSplitter ì´ˆê¸°í™” (Size: {settings.CHUNK_SIZE}, Overlap: {settings.CHUNK_OVERLAP})")
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""], # ë²•ê·œ êµ¬ì¡° ë³´ì¡´ì„ ìœ„í•œ êµ¬ë¶„ì ìˆœì„œ
            length_function=len,
        )

    def _calculate_hash(self, file_path: Path) -> str:
        """
        íŒŒì¼ì˜ SHA256 í•´ì‹œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.

        ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì²˜ë¦¬í•  ë•Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ìŠµë‹ˆë‹¤.

        Args:
            file_path (Path): íŒŒì¼ ê²½ë¡œ.

        Returns:
            str: 16ì§„ìˆ˜ SHA256 í•´ì‹œ ë¬¸ìì—´.
        """
        h = hashlib.sha256()
        with open(file_path, "rb") as f:
            while True:
                chunk = f.read(8192)
                if not chunk:
                    break
                h.update(chunk)
        return h.hexdigest()

    async def run(self, force_refresh: bool = False) -> None:
        """
        ë©”ì¸ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            force_refresh (bool): Trueì¼ ê²½ìš°, ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ì²˜ë¦¬ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•˜ê³ 
                                  ëª¨ë“  ë°ì´í„°ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì¸ë±ì‹±í•©ë‹ˆë‹¤.
        """
        if force_refresh:
            logger.warning("ğŸ”„ ê°•ì œ ìƒˆë¡œê³ ì¹¨ ëª¨ë“œ: ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            if settings.VECTOR_DB_PATH.exists():
                logger.warning(f"ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ ì¤‘: {settings.VECTOR_DB_PATH}")
                try:
                    shutil.rmtree(settings.VECTOR_DB_PATH)
                except PermissionError:
                    logger.error("âŒ ê¶Œí•œ ê±°ë¶€ë¨! Streamlit ì„œë²„ë¥¼ ì¤‘ì§€í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                    return
            self.db_state_manager.clear_all_status()

        # ë²¡í„° ì €ì¥ì†Œ ì´ˆê¸°í™” (ì—¬ê¸°ì„œ ì—°ê²° ìˆ˜ë¦½)
        self.vstore = VectorStoreManager()

        # 1. Scan for Target Files
        target_files: List[Path] = []
        
        # [FMVSS]: Use pre-processed JSON files (Data quality is better than raw XML)
        json_dir = settings.DATA_DIR / "processed_json_for_rag"
        if json_dir.exists():
            target_files.extend(json_dir.glob("*.json"))

        # [KMVSS]: Use raw XML files
        if settings.RAW_XML_KMVSS_PATH.exists():
            target_files.extend(settings.RAW_XML_KMVSS_PATH.glob("*.xml"))
            
        # [ECE]: Use PDF files
        if settings.RAW_PDF_ECE_PATH.exists():
            target_files.extend(settings.RAW_PDF_ECE_PATH.glob("*.pdf"))
        
        # Filter out archive files and sort for deterministic processing order
        target_files = sorted([f for f in target_files if "archive" not in str(f)])

        if not target_files:
            logger.warning("âŒ ì²˜ë¦¬í•  ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. íŒŒì¼ ìƒíƒœ í™•ì¸ (ì¦ë¶„ ì—…ë°ì´íŠ¸ ë¡œì§)
        last_states = self.db_state_manager.get_files_status(target_files)
        to_process: List[Tuple[Path, str]] = []

        for f in target_files:
            current_hash = self._calculate_hash(f)
            last_record = last_states.get(str(f))

            # ì²˜ë¦¬ ëŒ€ìƒ: ìƒˆ íŒŒì¼ OR í•´ì‹œ ë³€ê²½ë¨ OR ì´ì „ ì‹œë„ ì‹¤íŒ¨
            if not last_record or \
               last_record.file_hash != current_hash or \
               last_record.status == StatusEnum.FAIL:
                to_process.append((f, current_hash))
        
        if not to_process:
            logger.info("âœ¨ ëª¨ë“  ë°ì´í„°ê°€ ìµœì‹ ì…ë‹ˆë‹¤.")
            return

        logger.info(f"ğŸ”„ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ìˆ˜: {len(to_process)}ê°œ")
        
        # 3. íŒŒì¼ ì²˜ë¦¬ (ë¡œë“œ -> ì²­í‚¹ -> ì¸ë±ì‹±)
        documents_to_add: List[Document] = []
        processed_files: List[Tuple[Path, str, StatusEnum, Optional[str]]] = []

        for file_path, file_hash in to_process:
            error_msg: Optional[str] = None
            try:
                # íŒ©í† ë¦¬ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ í™•ì¥ìì— ë§ëŠ” ë¡œë” ìƒì„±
                loader = LoaderFactory.create(file_path)
                ingested_docs = await loader.load(file_path)
                
                if not ingested_docs:
                    logger.warning(f"âš ï¸ ë¬¸ì„œê°€ ì¶”ì¶œë˜ì§€ ì•ŠìŒ (Skipped): {file_path.name}")
                    # ì‹¤íŒ¨ê°€ ì•„ë‹Œ 'SKIPPED' ìƒíƒœë¡œ ê¸°ë¡í•˜ê±°ë‚˜, ì¼ë‹¨ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬í•˜ë˜ ë‚´ìš©ì€ ì—†ìŒ
                    processed_files.append((file_path, file_hash, StatusEnum.SUCCESS, "Skipped (No Content)"))
                    continue

                # ë‚´ë¶€ ë¬¸ì„œ í˜•ì‹ì„ LangChain í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                langchain_docs = [i_doc.to_langchain_format() for i_doc in ingested_docs]
                documents_to_add.extend(langchain_docs)
                
                processed_files.append((file_path, file_hash, StatusEnum.SUCCESS, None))
                logger.info(f"âœ… ë¡œë“œ ì„±ê³µ: {file_path.name}")

            except Exception as e:
                error_msg = f"{type(e).__name__}: {e}"
                logger.error(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path.name}): {error_msg}")
                processed_files.append((file_path, file_hash, StatusEnum.FAIL, error_msg))

        if documents_to_add:
            # 3-1. ì˜ë¯¸ë¡ ì  ì²­í‚¹ ë° ë¬¸ë§¥ ì£¼ì…
            logger.info(f"âœ‚ï¸ {len(documents_to_add)}ê°œ ë¬¸ì„œ ë¶„í•  ì¤‘...")
            chunks = self.splitter.split_documents(documents_to_add)
            logger.info(f"ğŸ§© {len(chunks)}ê°œ ì²­í¬ ìƒì„±ë¨.")
            
            enriched_chunks = self._enrich_chunks_context(chunks)

            # 3-2. ë²¡í„° DBì— ì¸ë±ì‹±
            self.vstore.add_documents(enriched_chunks)

        # 4. ìƒíƒœ ë°ì´í„°ë² ì´ìŠ¤ ì—…ë°ì´íŠ¸ (ì›ìì  ì—…ë°ì´íŠ¸)
        for file_path, file_hash, status, error_msg in processed_files:
            self.db_state_manager.update_status(file_path, file_hash, status, error_msg)

        logger.info("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    def _enrich_chunks_context(self, chunks: List[Document]) -> List[Document]:
        """
        ê° ì²­í¬ì˜ í˜ì´ì§€ ì½˜í…ì¸ ì— ë©”íƒ€ë°ì´í„° ë¬¸ë§¥ì„ ì£¼ì…í•©ë‹ˆë‹¤.
        
        ì²­í¬ í…ìŠ¤íŠ¸ ìì²´ê°€ ì¼ë°˜ì ì´ë”ë¼ë„ ê·œì • ID ë° ì œëª©ê³¼ ê°™ì€ 
        ì¤‘ìš”í•œ ë¬¸ë§¥ ì •ë³´ë¥¼ ì„ë² ë”© ë²¡í„°ì— í¬í•¨ì‹œì¼œ ê²€ìƒ‰ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

        Args:
            chunks (List[Document]): ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸.

        Returns:
            List[Document]: ë¬¸ë§¥ì´ ì£¼ì…ëœ ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸.
        """
        enriched = []
        for chunk in chunks:
            source = chunk.metadata.get("source", "")
            title = chunk.metadata.get("title", "")
            std_id = chunk.metadata.get("standard_id", "")

            # ëŒ€ì²´ ë¡œì§: ë©”íƒ€ë°ì´í„°ì— IDê°€ ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ
            if not std_id and source:
                path_obj = Path(source)
                std_id = path_obj.stem

            # ë¬¸ë§¥ í—¤ë” ìƒì„±
            context_header = f"[Standard: {std_id}]"
            if title:
                context_header += f" [Title: {title}]"

            # ì½˜í…ì¸  ì•ì— ë¬¸ë§¥ ì¶”ê°€
            chunk.page_content = f"{context_header}\n{chunk.page_content}"
            enriched.append(chunk)

        logger.info(f"ğŸ§¬ {len(enriched)}ê°œ ì²­í¬ì— ë©”íƒ€ë°ì´í„° ë¬¸ë§¥ ì£¼ì… ì™„ë£Œ.")
        return enriched


if __name__ == "__main__":
    pipeline = IngestionPipeline()
    asyncio.run(pipeline.run(force_refresh=True))